# Dockerfile.minimal

# 1) Start with a minimal Python 3.9
FROM python:3.9-slim

# 2) Install system dependencies required by faiss and possibly PyTorch
RUN apt-get update && apt-get install -y --no-install-recommends \
    g++ \
    libblas-dev \
    liblapack-dev \
    ca-certificates \
 && rm -rf /var/lib/apt/lists/*

# 3) Install Python packages:
#    - PyTorch (CPU-only)
#    - Transformers
#    - FAISS CPU
#    - sentence-transformers
#    - sagemaker-inference (to handle /invocations requests in SageMaker)
RUN pip install --no-cache-dir \
    torch==2.0.1 \
    transformers==4.30.2 \
    faiss-cpu==1.7.3 \
    sentence-transformers==2.2.2 \
    sagemaker-inference==1.9.1

# 4) Copy your inference.py to the code directory
COPY inference.py /opt/ml/model/code/inference.py

# 5) (Optional) environment variables
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE

# For usage in your script:
ENV FAISS_INDEX_S3="s3://aym-client-data-in/rag/faiss_index.bin"
ENV METADATA_S3="s3://aym-client-data-in/rag/index_metadata.json"
ENV EMBED_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2"
ENV GEN_MODEL_NAME="my_summarization_model"

# 6) The sagemaker-inference library uses environment variables to find your script
#    By default, it looks for "predict_fn" in /opt/ml/model/code/inference.py
#    We'll declare them here or in the SageMaker Model container environment.
ENV SAGEMAKER_PROGRAM=inference.py
ENV SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/model/code

# We'll rely on the sagemaker-inference default entrypoint & CMD
# so no further CMD or ENTRYPOINT lines are needed.

