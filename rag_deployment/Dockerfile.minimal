# Dockerfile.minimal

FROM python:3.9-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    g++ \
    libblas-dev \
    liblapack-dev \
    ca-certificates \
 && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    torch==2.0.1 \
    transformers==4.30.2 \
    faiss-cpu==1.7.3 \
    sentence-transformers==2.2.2 \
    sagemaker-inference==1.9.1

COPY inference.py /opt/ml/model/code/inference.py

ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV FAISS_INDEX_S3="s3://aym-client-data-in/rag/faiss_index.bin"
ENV METADATA_S3="s3://aym-client-data-in/rag/index_metadata.json"
ENV EMBED_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2"
ENV GEN_MODEL_NAME="my_summarization_model"

# Tells the sagemaker-inference container that "model_fn" is in inference.py
ENV SAGEMAKER_PROGRAM=inference.py
ENV SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/model/code

# No custom ENTRYPOINT, so it uses sagemaker-inference's default
# which calls model_fn at startup, then calls predict_fn for each request.

